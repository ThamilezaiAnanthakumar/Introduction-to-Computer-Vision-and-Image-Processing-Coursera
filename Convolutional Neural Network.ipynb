{"cells":[{"cell_type":"markdown","id":"7a6ab7f5-1da6-4f96-9c27-2bea010833b7","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"fd4264c4-2788-434f-8142-8d9dba52f235","metadata":{},"outputs":[],"source":["<h1>Convolutional Neural Network</h1> \n"]},{"cell_type":"markdown","id":"448382e2-aa63-4ab3-8571-38405f45976f","metadata":{},"outputs":[],"source":["\n","<h3>Objective for this Notebook<h3>    \n","<h5> 1. Learn how to use a Convolutional Neural Network to classify handwritten digits from the MNIST database</h5>\n","<h5> 2. Learn how to reshape the images to make them faster to process </h5>     \n","\n"]},{"cell_type":"markdown","id":"d1a641b4-e865-4379-a5a1-11a99571af6a","metadata":{},"outputs":[],"source":["<h2>Table of Contents</h2>\n","<p>In this lab, we will use a Convolutional Neural Network to classify handwritten digits from the MNIST database. We will reshape the images to make them faster to process </p>\n","\n","<ul>\n","<li><a href=\"#Makeup_Data\">Get Some Data</a></li>\n","<li><a href=\"#CNN\">Convolutional Neural Network</a></li>\n","<li><a href=\"#Train\">Define Softmax, Criterion function, Optimizer, and Train the Model</a></li>\n","<li><a href=\"#Result\">Analyze Results</a></li>\n","</ul>\n","<p>Estimated Time Needed: <strong>25 min</strong> 14 min to train model </p>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"af3b51b2-141c-473a-9740-457feb0fb160","metadata":{},"outputs":[],"source":["<h2>Preparation</h2>\n"]},{"cell_type":"code","id":"e025bc4a-83cc-4e87-b8a5-d4276d98ac74","metadata":{},"outputs":[],"source":["!pip3 install torch torchvision torchaudio"]},{"cell_type":"code","id":"cb6d45f1-bbf3-41dd-b865-c5ff08ce43d5","metadata":{},"outputs":[],"source":["# Import the libraries we need to use in this lab\n\n# Using the following line code to install the torchvision library\n# !conda install -y torchvision\n\n# PyTorch Library\nimport torch\n# PyTorch Neural Network\nimport torch.nn as nn\n# Allows us to transform data\nimport torchvision.transforms as transforms\n# Allows us to download the dataset\nimport torchvision.datasets as dsets\n# Used to graph data and loss curves\nimport matplotlib.pylab as plt\n# Allows us to use arrays to manipulate and store data\nimport numpy as np"]},{"cell_type":"markdown","id":"b08b9df2-7203-4610-8c61-a1a73a3bae26","metadata":{},"outputs":[],"source":["Define the function <code>plot_channels</code> to plot out the kernel parameters of  each channel \n"]},{"cell_type":"code","id":"ab7ddf96-d041-475c-8e55-f9289fe6cb95","metadata":{},"outputs":[],"source":["# Define the function for plotting the channels\n\ndef plot_channels(W):\n    n_out = W.shape[0]\n    n_in = W.shape[1]\n    w_min = W.min().item()\n    w_max = W.max().item()\n    fig, axes = plt.subplots(n_out, n_in)\n    fig.subplots_adjust(hspace=0.1)\n    out_index = 0\n    in_index = 0\n    \n    #plot outputs as rows inputs as columns \n    for ax in axes.flat:\n        if in_index > n_in-1:\n            out_index = out_index + 1\n            in_index = 0\n        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])\n        in_index = in_index + 1\n\n    plt.show()"]},{"cell_type":"markdown","id":"a46b0daa-1353-4246-b4a3-d00ceafc2cac","metadata":{},"outputs":[],"source":["Define the function <code>plot_parameters</code> to plot out the kernel parameters of each channel with Multiple outputs. \n"]},{"cell_type":"code","id":"78750220-b949-4b17-84e3-7c7e764ed2dd","metadata":{},"outputs":[],"source":["# Define the function for plotting the parameters\n\ndef plot_parameters(W, number_rows=1, name=\"\", i=0):\n    W = W.data[:, i, :, :]\n    n_filters = W.shape[0]\n    w_min = W.min().item()\n    w_max = W.max().item()\n    fig, axes = plt.subplots(number_rows, n_filters // number_rows)\n    fig.subplots_adjust(hspace=0.4)\n\n    for i, ax in enumerate(axes.flat):\n        if i < n_filters:\n            # Set the label for the sub-plot.\n            ax.set_xlabel(\"kernel:{0}\".format(i + 1))\n\n            # Plot the image.\n            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')\n            ax.set_xticks([])\n            ax.set_yticks([])\n    plt.suptitle(name, fontsize=10)    \n    plt.show()"]},{"cell_type":"markdown","id":"cbee497c-6d5d-485e-a6d6-f631efcaf59c","metadata":{},"outputs":[],"source":["Define the function <code>plot_activation</code> to plot out the activations of the Convolutional layers  \n"]},{"cell_type":"code","id":"f8bd0940-600c-4715-9a2f-a0d2b9c02482","metadata":{},"outputs":[],"source":["# Define the function for plotting the activations\n\ndef plot_activations(A, number_rows=1, name=\"\", i=0):\n    A = A[0, :, :, :].detach().numpy()\n    n_activations = A.shape[0]\n    A_min = A.min().item()\n    A_max = A.max().item()\n    fig, axes = plt.subplots(number_rows, n_activations // number_rows)\n    fig.subplots_adjust(hspace = 0.9)    \n\n    for i, ax in enumerate(axes.flat):\n        if i < n_activations:\n            # Set the label for the sub-plot.\n            ax.set_xlabel(\"activation:{0}\".format(i + 1))\n\n            # Plot the image.\n            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')\n            ax.set_xticks([])\n            ax.set_yticks([])\n    plt.show()"]},{"cell_type":"markdown","id":"6c1baae4-92e8-401c-9d7c-61d911f08b9d","metadata":{},"outputs":[],"source":["Define the function <code>show_data</code> to plot out data samples as images.\n"]},{"cell_type":"code","id":"e35d203e-1a01-427f-be62-373386c6df5d","metadata":{},"outputs":[],"source":["def show_data(data_sample):\n    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n    plt.title('y = '+ str(data_sample[1]))"]},{"cell_type":"markdown","id":"a87da502-3611-4468-b938-f6031a528e2f","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"8922fa8c-3c97-481d-866a-f13873efe383","metadata":{},"outputs":[],"source":["<h2 id=\"Makeup_Data\">Get the Data</h2> \n"]},{"cell_type":"markdown","id":"c3eae86d-269e-4fc1-b01a-a775285faca9","metadata":{},"outputs":[],"source":["We create a transform to resize the image and convert it to a tensor.\n"]},{"cell_type":"code","id":"e14b52ed-30fb-4cc4-8d36-bcd6cbccb909","metadata":{},"outputs":[],"source":["IMAGE_SIZE = 16\n\n# First the image is resized then converted to a tensor\ncomposed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"]},{"cell_type":"markdown","id":"a5191bfd-b8e9-473b-b6bf-6f3dbf3b403d","metadata":{},"outputs":[],"source":["Load the training dataset by setting the parameter <code>train</code> to <code>True</code>. We use the transform defined above.\n"]},{"cell_type":"code","id":"2917895e-cd5a-4963-9d77-d45a26d9e10d","metadata":{},"outputs":[],"source":["train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)"]},{"cell_type":"markdown","id":"2a414d4e-c103-4b2f-9873-7a1e35f5cde8","metadata":{},"outputs":[],"source":["Load the testing dataset by setting the parameter train to <code>False</code>.\n"]},{"cell_type":"code","id":"356e9d82-36c1-4956-aae2-0b04b3b388e4","metadata":{},"outputs":[],"source":["validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)"]},{"cell_type":"markdown","id":"4b1b9bc5-1a0c-4158-ac28-0dca444490b4","metadata":{},"outputs":[],"source":["Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.\n"]},{"cell_type":"markdown","id":"ebea1595-a718-437e-9eee-cb9eff9a9f34","metadata":{},"outputs":[],"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.2.1imagenet.png\" width=\"550\" alt=\"MNIST data image\">\n"]},{"cell_type":"markdown","id":"27eef359-67bb-4388-a5b7-d09d952ffc84","metadata":{},"outputs":[],"source":["Print out the fourth label \n"]},{"cell_type":"code","id":"373ffcc8-f6ea-4079-89f8-2b3002ea9308","metadata":{},"outputs":[],"source":["# The label for the fourth data element\n\ntrain_dataset[3][1]"]},{"cell_type":"markdown","id":"a9270616-b16e-4f23-9ac6-849f4f76d6b5","metadata":{},"outputs":[],"source":["Plot the fourth sample \n"]},{"cell_type":"code","id":"a7866610-cd9a-48c0-90d5-1780518fed2a","metadata":{},"outputs":[],"source":["# The image for the fourth data element\nshow_data(train_dataset[3])"]},{"cell_type":"markdown","id":"16b38bc8-2c48-4aeb-8241-0db30d0d6e8f","metadata":{},"outputs":[],"source":["The fourth sample is a \"1\".\n"]},{"cell_type":"markdown","id":"d558c5a6-1f8c-4d05-856e-4a4329d58918","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"40ed8bca-f351-40ec-a1fb-b9fe0768b3bd","metadata":{},"outputs":[],"source":["<h2 id=\"CNN\">Build a Convolutional Neural Network Class</h2>\n"]},{"cell_type":"markdown","id":"6df4011b-1646-4474-af1c-e9b5e883b155","metadata":{},"outputs":[],"source":["Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layers.\n","\n","Channel Width can be calculated using the following formula\n","\n","![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-CV0101EN-Coursera/labs/Module4/Channel_Width.png)\n","\n","Channel width must be calculated after each CNN layer and Max Pool layer.\n","\n","Default Values CNN Layer:\n","\n","Stride: 1\n","Padding: 0\n","Dilation: 1\n","\n","Default Values Max Pool Layer:\n","\n","Stride: Kernel Size\n","Padding: 0\n","Dilation: 1\n"]},{"cell_type":"code","id":"861b5e3f-f2d7-48c6-999e-35cd538b103d","metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n    \n    # Contructor\n    def __init__(self, out_1=16, out_2=32):\n        super(CNN, self).__init__()\n        # The reason we start with 1 channel is because we have a single black and white image\n        # Channel Width after this layer is 16\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n        # Channel Wifth after this layer is 8\n        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n        \n        # Channel Width after this layer is 8\n        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n        # Channel Width after this layer is 4\n        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n        # In total we have out_2 (32) channels which are each 4 * 4 in size based on the width calculation above. Channels are squares.\n        # The output is a value for each class\n        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n    \n    # Prediction\n    def forward(self, x):\n        # Puts the X value through each cnn, relu, and pooling layer and it is flattened for input into the fully connected layer\n        x = self.cnn1(x)\n        x = torch.relu(x)\n        x = self.maxpool1(x)\n        x = self.cnn2(x)\n        x = torch.relu(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        return x\n    \n    # Outputs result of each stage of the CNN, relu, and pooling layers\n    def activations(self, x):\n        # Outputs activation this is not necessary\n        z1 = self.cnn1(x)\n        a1 = torch.relu(z1)\n        out = self.maxpool1(a1)\n        \n        z2 = self.cnn2(out)\n        a2 = torch.relu(z2)\n        out1 = self.maxpool2(a2)\n        out = out.view(out.size(0),-1)\n        return z1, a1, z2, a2, out1,out"]},{"cell_type":"markdown","id":"54ab45be-75e7-484c-8d09-46ed92a2d718","metadata":{},"outputs":[],"source":["<h2 id=\"Train\">Define the Convolutional Neural Network Classifier, Criterion function, Optimizer, and Train the Model</h2> \n"]},{"cell_type":"markdown","id":"6be86fb7-61d1-4beb-8d81-565214f28d8d","metadata":{},"outputs":[],"source":["There are 16 output channels for the first layer, and 32 output channels for the second layer \n"]},{"cell_type":"code","id":"fc7a1d7f-7ca1-4df6-9fd2-268c892ce8e6","metadata":{},"outputs":[],"source":["# Create the model object using CNN class\n\nmodel = CNN(out_1=16, out_2=32)"]},{"cell_type":"markdown","id":"f7407a0c-6fd9-44e7-b095-b5e61f64797d","metadata":{},"outputs":[],"source":["Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"]},{"cell_type":"code","id":"7da6caae-049d-48c9-b85f-740f655e3d16","metadata":{},"outputs":[],"source":["# Plot the parameters\n\nplot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \")\nplot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )"]},{"cell_type":"markdown","id":"6d1d18e9-ff3c-4816-9f69-0af56e1226cd","metadata":{},"outputs":[],"source":["Define the loss function, the optimizer, and the dataset loader \n"]},{"cell_type":"code","id":"0ccfc891-0807-45c6-b301-1aab1b7e9281","metadata":{},"outputs":[],"source":["# We create a criterion which will measure loss\ncriterion = nn.CrossEntropyLoss()\nlearning_rate = 0.1\n# Create an optimizer that updates model parameters using the learning rate and gradient\noptimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n# Create a Data Loader for the training data with a batch size of 100 \ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n# Create a Data Loader for the validation data with a batch size of 5000 \nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"]},{"cell_type":"markdown","id":"f244cc39-668c-40ed-b5b2-04af26108e99","metadata":{},"outputs":[],"source":["Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"]},{"cell_type":"code","id":"57e3997e-7c90-4982-8f27-1b264d7b1a6d","metadata":{},"outputs":[],"source":["# Train the model\n\n# Number of times we want to train on the taining dataset\nn_epochs=3\n# List to keep track of cost and accuracy\ncost_list=[]\naccuracy_list=[]\n# Size of the validation dataset\nN_test=len(validation_dataset)\n\n# Model Training Function\ndef train_model(n_epochs):\n    # Loops for each epoch\n    for epoch in range(n_epochs):\n        # Keeps track of cost for each epoch\n        COST=0\n        # For each batch in train loader\n        for x, y in train_loader:\n            # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n            optimizer.zero_grad()\n            # Makes a prediction based on X value\n            z = model(x)\n            # Measures the loss between prediction and acutal Y value\n            loss = criterion(z, y)\n            # Calculates the gradient value with respect to each weight and bias\n            loss.backward()\n            # Updates the weight and bias according to calculated gradient value\n            optimizer.step()\n            # Cumulates loss \n            COST+=loss.data\n        \n        # Saves cost of training data of epoch\n        cost_list.append(COST)\n        # Keeps track of correct predictions\n        correct=0\n        # Perform a prediction on the validation  data  \n        for x_test, y_test in validation_loader:\n            # Makes a prediction\n            z = model(x_test)\n            # The class with the max value is the one we are predicting\n            _, yhat = torch.max(z.data, 1)\n            # Checks if the prediction matches the actual value\n            correct += (yhat == y_test).sum().item()\n        \n        # Calcualtes accuracy and saves it\n        accuracy = correct / N_test\n        accuracy_list.append(accuracy)\n     \ntrain_model(n_epochs)"]},{"cell_type":"markdown","id":"e5c17172-f240-4fa6-a73d-b583532a9d5f","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"587567fa-a65e-43a5-9e60-df9ef1239546","metadata":{},"outputs":[],"source":["<h2 id=\"Result\">Analyze Results</h2> \n"]},{"cell_type":"markdown","id":"5d1123f0-e4c0-46b1-bbc7-760533c18ef8","metadata":{},"outputs":[],"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","id":"1ac4cc95-7b5a-4570-853d-dd36c711496f","metadata":{},"outputs":[],"source":["# Plot the Loss and Accuracy vs Epoch graph\n\nfig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(cost_list, color=color)\nax1.set_xlabel('epoch', color=color)\nax1.set_ylabel('Cost', color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color) \nax2.set_xlabel('epoch', color=color)\nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', color=color)\nfig.tight_layout()"]},{"cell_type":"markdown","id":"313b0b04-bf9e-42a2-a1e2-8ee5224c7368","metadata":{},"outputs":[],"source":["View the results of the parameters for the Convolutional layers \n"]},{"cell_type":"code","id":"4447ffe8-e41c-490f-875b-d6dc56c097b2","metadata":{},"outputs":[],"source":["# Plot the channels\n\nplot_channels(model.state_dict()['cnn1.weight'])\nplot_channels(model.state_dict()['cnn2.weight'])"]},{"cell_type":"markdown","id":"b301195f-eff5-40c8-be0b-209340e15a8e","metadata":{},"outputs":[],"source":["Consider the following sample \n"]},{"cell_type":"code","id":"a1353201-7424-4654-aaf0-48a6eb9c08c9","metadata":{},"outputs":[],"source":["# Show the second image\n\nshow_data(train_dataset[1])"]},{"cell_type":"markdown","id":"6ca54b05-cbaf-4053-a066-6f438e52aff3","metadata":{},"outputs":[],"source":["Determine the activations \n"]},{"cell_type":"code","id":"9dd5ba23-356c-457f-9dc4-780598847a04","metadata":{},"outputs":[],"source":["# Use the CNN activations class to see the steps\n\nout = model.activations(train_dataset[1][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))"]},{"cell_type":"markdown","id":"898c3846-9ad0-4c23-b954-d74a1699cc27","metadata":{},"outputs":[],"source":["Plot out the first set of activations \n"]},{"cell_type":"code","id":"8cac6295-9024-4f00-b26b-7b87ae8b9300","metadata":{},"outputs":[],"source":["# Plot the outputs after the first CNN\n\nplot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\")"]},{"cell_type":"markdown","id":"395f5b4a-af17-4ad2-8101-ba9a11b5542d","metadata":{},"outputs":[],"source":["The image below is the result after applying the relu activation function \n"]},{"cell_type":"code","id":"a741c7f3-593c-4a1a-b2ea-d83a4ab23d29","metadata":{},"outputs":[],"source":["# Plot the outputs after the first Relu\n\nplot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\")"]},{"cell_type":"markdown","id":"538671a5-7d36-4a3f-bd1b-af1dbd89382f","metadata":{},"outputs":[],"source":["The image below is the result of the activation map after the second output layer.\n"]},{"cell_type":"code","id":"4c598c68-fc66-4610-934a-d411365e4dd7","metadata":{},"outputs":[],"source":["# Plot the outputs after the second CNN\n\nplot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\")"]},{"cell_type":"markdown","id":"3d8fbbdd-0efe-4467-8694-d2f7bfceaa2a","metadata":{},"outputs":[],"source":["The image below is the result of the activation map after applying the second relu  \n"]},{"cell_type":"code","id":"2a134c7d-0ab6-415d-8350-279e95716c40","metadata":{},"outputs":[],"source":["# Plot the outputs after the second Relu\n\nplot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\")"]},{"cell_type":"markdown","id":"e579b6e6-2df1-4eae-a31e-f6617e427534","metadata":{},"outputs":[],"source":["We can  see the result for the third sample \n"]},{"cell_type":"code","id":"aa7aa7e2-517e-4851-a1ad-d8174a182bb5","metadata":{},"outputs":[],"source":["# Show the third image\n\nshow_data(train_dataset[2])"]},{"cell_type":"code","id":"fb359bc1-16b3-4b03-a1c2-0f196c585f59","metadata":{},"outputs":[],"source":["# Use the CNN activations class to see the steps\n\nout = model.activations(train_dataset[2][0].view(1, 1, IMAGE_SIZE, IMAGE_SIZE))"]},{"cell_type":"code","id":"62b4fea1-51d9-4bae-9346-3dbe21b58fd2","metadata":{},"outputs":[],"source":["# Plot the outputs after the first CNN\n\nplot_activations(out[0], number_rows=4, name=\"Output after the 1st CNN\")"]},{"cell_type":"code","id":"b289db6a-c2e9-4020-b56e-ceffd598ffda","metadata":{},"outputs":[],"source":["# Plot the outputs after the first Relu\n\nplot_activations(out[1], number_rows=4, name=\"Output after the 1st Relu\")"]},{"cell_type":"code","id":"5a64616d-9b49-46eb-ae6b-3a950d7af291","metadata":{},"outputs":[],"source":["# Plot the outputs after the second CNN\n\nplot_activations(out[2], number_rows=32 // 4, name=\"Output after the 2nd CNN\")"]},{"cell_type":"code","id":"64fe5134-2c42-4007-bbe5-ace5ffcca8f2","metadata":{},"outputs":[],"source":["# Plot the outputs after the second Relu\n\nplot_activations(out[3], number_rows=4, name=\"Output after the 2nd Relu\")"]},{"cell_type":"markdown","id":"d4539afc-d492-47e5-9eb5-96ea5e3ee87b","metadata":{},"outputs":[],"source":["Plot the first five misclassified samples:\n"]},{"cell_type":"code","id":"ed378092-1b5a-467c-8f4c-02844cb65e69","metadata":{},"outputs":[],"source":["# Plot the misclassified samples\n\ncount = 0\nfor x, y in torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=1):\n    z = model(x)\n    _, yhat = torch.max(z, 1)\n    if yhat != y:\n        show_data((x, y))\n        plt.show()\n        print(\"yhat: \",yhat)\n        count += 1\n    if count >= 5:\n        break  "]},{"cell_type":"markdown","id":"a46ad21f-d8fb-4709-ab3f-213e789dc963","metadata":{},"outputs":[],"source":["\n","<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-CV0101EN-Coursera&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n"]},{"cell_type":"markdown","id":"7a24fc0e-f3a7-4079-a4c0-399d6b8582eb","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"1dd52728-d419-44ca-bd65-cb3a50540154","metadata":{},"outputs":[],"source":["<h2>About the Authors:</h2> \n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","id":"707716c4-0325-4765-b45d-c3b473108088","metadata":{},"outputs":[],"source":["Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"https://ca.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>\n"]},{"cell_type":"markdown","id":"8356ac91-7331-4444-b0ee-eb34d4d9ed3f","metadata":{},"outputs":[],"source":["Thanks to Magnus <a href=\"http://www.hvass-labs.org/\">Erik Hvass Pedersen</a> whose tutorials helped me understand Convolutional Neural Networks.\n"]},{"cell_type":"markdown","id":"0354d945-0dec-4b98-a956-cd7ce708c0b7","metadata":{},"outputs":[],"source":["\n","<!--## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","-->\n","\n","\n","<hr>\n","\n","## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"6b91f85eeaadd7383171914aac4283eee4b7dd364bd84e82ed31d5575a885b59"},"nbformat":4,"nbformat_minor":4}